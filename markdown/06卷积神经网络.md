# 卷积神经网络

## 1 从全连接层到卷积

能够用卷积而不是全连接层有以下几个原因

1. 不变性：某个物体在图像中更换位置应该不改变特性

具体的公式推导可以参考书上内容，全连接层的权重维度是 `输入维度 * 输出维度`

这也就是说，负责输入的那几个维度都是共享的参数，可以全部压缩到一组值 

$h_{i,j} = \sum_{a,b} v_{i,j,a,b}\, x_{i+a,\,j+b}$ 中我们可以去除所有的关于 a 和 b 的信息（$v_{i,j,a,b} = v_{a,b}$），因为不管偏移是多少输出都是一致的

随后可以得到 $h_{i,j} = \sum_{a,b} v_{a,b}\, x_{i+a,\,j+b}$

2. 局部性：不需要关注全局性的信息，远离我的输入位置的内容我不需要关系

$h_{i,j}=\sum_{a=-\Delta}^{\Delta}\sum_{b=-\Delta}^{\Delta}v_{a,b} x_{i+a,j+b}$ 也就是在这个范围内才考虑，最后就得到了二维互相关

卷积实际上就是把这个正向改成倒过来（实际计算的时候是交叉相关，不是卷积，是正向计算的

## 2 填充与步幅

### 2.1 Padding 填充

Padding 解决的是卷积后图像尺寸下降太快的问题

对于一个 边长为 $N$ 的图像使用 $K$ 大小的卷积核，经过一次后得到的是 $ N - K + 1$ 经过几层以后可能图像就变得很小了，这导致了我们的网络会变得很浅，所以我们需要填充

Padding 就是在图像的最外圈提供一个填充层，填充一般值是 $P = K - 1$ ，这样就可以正好让输入和输出相等

这里假设的是所有位置的 Pedding ，而不是某一侧的 Pedding

一般而言我们的 $N$ 都是奇数，所以对称执行上下左右填充就行了

### 2.2 Stride 步幅

Stride 解决的是卷积后图像尺寸下降太快的问题

控制每一次移动的间隔不是 1 ，而是一个给定的值（横纵可以用不同的步幅）

假设 步幅 $S$ 则可以得到实际输出形状是 $\lfloor (N - K + P) / S + 1 \rfloor$ （向下取整的意思，卷积导致的那个 $+ 1$ 在外面了

## 3 多通道数

### 3.1 简介

对于图像一般有 RGB 三个通道的情况下，我们有以下几种方式

1. 使用灰度图像，但对于复杂的情况可能效果并不好

2. 多输入通道：每个通道一个卷积核，结果是所有通道的卷积的和

 - 输入图像：$\mathbf{X} \in \mathbb{R}^{c_i \times n_h \times n_w}$

 - 卷积核 ：$\mathbf{W} \in \mathbb{R}^{c_i \times k_h \times k_w}$

 - 输出结果：$\mathbf{Y} \in \mathbb{R}^{m_h \times m_w}$ （多通道合成为一个通道，使用不同通道和相加）

3. 多输出通道：用多个三维卷积，每一个核生成一个输出通道

 - 输入 $\mathbf{X} \in \mathbb{R}^{c_i \times n_h \times n_w}$

 - 卷积核 $\mathbf{W} \in \mathbb{R}^{c_o \times c_i \times k_h \times k_w}$

 - 输出 $\mathbf{Y} \in \mathbb{R}^{c_o \times m_h \times m_w}$

多输出通道：不同的卷积核可以匹配不同的模式，也可以匹配不同的边缘等内容

多输入通道：整合多个通道模式的数据来完成组合实习

### 3.2 1 x 1 卷积

$W \in \mathbb{R}^{C_{out}\times C_{in}\times 1\times1}$ 这个卷积核不会识别空间信息，但是可以融合多个通道的信息 

$C_{in}$ 就是输入的通道数，不同的通道卷积核的值可能不同，让不同通道计算加权和

$C_{out}$ 是我们期望的输出通道数，一个通道就需要一个 $C_{out}$ 也就是一组卷积

**事实上 1×1 卷积就是：在每个像素位置上做一次全连接，并且所有位置共享同一组权重**

普通的线性全连接： $\mathbf{y} = \mathbf{W}\mathbf{x} + \mathbf{b},\quad \mathbf{W}\in\mathbb{R}^{D_{out}\times D_{in}}$

1x1 卷积的行为：$\mathbf{y}_{i,j} = \mathbf{W}\mathbf{x}_{i,j} + \mathbf{b},\quad \mathbf{W}\in\mathbb{R}^{C_{out}\times C_{in}}$

| **全连接（FC）** | **1×1 卷积** |
| ----------------------------------------------------- | -------------------------------------------------------- |
| 输入向量 $\mathbf{x}\in\mathbb{R}^{D_{in}}$ | 像素处通道向量$ \mathbf{x}_{i,j}\in\mathbb{R}^{C_{in}}$ |
| 权重 $\mathbf{W}\in\mathbb{R}^{D_{out}\times D_{in}}$ | 权重 $\mathbf{W}\in\mathbb{R}^{C_{out}\times C_{in}}$ |
| 输出向量 $\mathbf{y}\in\mathbb{R}^{D_{out}}$ | 像素处输出向量 $\mathbf{y}_{i,j}\in\mathbb{R}^{C_{out}}$ |
| 一次线性映射 | **在每个像素重复的线性映射** |
| 不共享参数 | **跨空间位置共享参数** |

### 3.3 列举例子

我们使用 $X \in \mathbb{R}^{3 \times 10 \times 10}$ 作为输入图像，使用 $W \in \mathbb{R}^{3 \times 3 \times 3}$ 作为卷积核

根据 公式 $\lfloor (N - K + P) / S + 1 \rfloor$ 计算，实际得到为 $1 \times 8 \times 8$ 的结果，因为卷积核只有一组

假如我们使用了 4 组卷积核，也就是增加了一个维度：$W \in \mathbb{R}^{4 \times 3 \times 3 \times 3}$

最后会得到 4 个 $1 \times 8 \times 8$ 合并一下 channel 就是 $4 \times 8 \times 8$

### 3.4 计算复杂度

输入通道数：$c_i$, 输出通道数：$c_o$, 卷积核尺寸：$k_h$, $k_w$, 输出特征图空间尺寸：$m_h$, $m_w$

乘法计算次数：$\text{Mul} = c_o \cdot m_h \cdot m_w \cdot \boxed{ c_i \cdot k_h \cdot k_w}$ 其中框起来的是某一个像素所需的计算量 

加法计算次数：$\text{Add} = c_o \cdot m_h \cdot m_w \cdot (c_i \cdot k_h \cdot k_w - 1)$

## 4 池化层

卷积对位置信息是很敏感的，并没有很好的平移不变形，因此我们需要 Pooling 操作

与卷积不同的是，最大池化层的工作是将窗口中的最大值设置到输出结果中

操作也和卷积比较类似，有对应的填充以及步幅，不同的是没有可以学习的参数，输出输出的通道数是完全一致的

池化层常用的有：

- 最大池化层：每一个窗口最强的模式信号

- 平均池化层：把最大池化中的最大换成平均，抖动小一点

## 5 LeNet

LeNet 是用来识别手写数据集的一个网络

其实重点关注一下网络是怎么将输入的图像一点点压缩的

常用的卷积层，将输入高宽都缩小 1 倍，然后通道数增加 1 倍，实际上将信息压缩了 1 倍


## 6 有意思的 QA

1. 为什么 CNN 卷积核这么小也能捕捉图像特征？

因为卷积一旦层数量上来了就可以获取很大一个返回的信息，例如连续两层卷积核，上层卷积的一个矩阵值，是由下层卷积的一整个卷积核范围内的信息得到的，以此类推
