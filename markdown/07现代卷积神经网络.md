# 现代卷积神经网络

> 这一章节中不再手动写 python 的代码，因为都是网络结构上的改变
>
> 可以直接用 jupyter 中运行

## AlexNet

AlexNet 可以算是第一个真正出名的深度学习架构

AlexNet 在 LeNet 的基础上变得更深更大，同时做出了一些改进（ReLU, Dropout, MaxPooling，加上了数据增强），最主要的改进是方法论改变了，计算机视觉开始了端到端的方法，而不是人工提取特征了

![AlexNet](https://d2l.ai/_images/alexnet.svg)

## VGG

VGG 是一个设计思想更加优秀的一个网络，是一个使用块的网络

在 AlexNet 的基础上我们可以做：更多的全连接（代价昂贵）或者更多的卷积，或者使用将卷积层组合成块

VGG 使用了一个块，使用 3x3 的卷积（填充 1）执行 n 次，然后 2x2 池化（步幅2）执行一次

VGG 使用更深更大的 AlexNet，也就是块的堆叠



## NiN

全连接层的问题是参数是非常巨大的，计算代价昂贵，同时也非常容易过拟合

所以用 CNN 来替换 MLP 的线性层（也就是NiN块）：在一个卷积层后跟上两个 1x1 的卷积层，其实就相当于全连接层

最后通过全局的平均池化层来执行输出的结果

![NiN](https://d2l.ai/_images/nin.svg)

## GoogLeNet

第一个几乎超过 100 层的卷积神经网络

把各种各样的方式并行处理同一个输出，最后在输出时进行整合，用通道数的权重可以控制不同模块的重要性

![Inception 块](https://d2l.ai/_images/inception.svg)

Inception 块的输出通道来源多种多样，但是他的计算复杂度小得多（相对于纯粹的卷积来获取通道）

使用四条不同超参数的路径来抽取不同的信息，优势是模型参数小，计算复杂度低

## 有意思的 QA

1. 为什么超宽的CNN/FC不好用？

因为太宽的会导致很容易过拟合



