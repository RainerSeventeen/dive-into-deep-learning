# 数据处理

> 写一点官方文档中没有的，或者对自己来说相对更新的知识的内容

## 1 微积分

### 1.1 矩阵相关

1. 列向量对标量求导，得到一个行向量

$$
\frac{\partial y}{\partial x} =
\begin{bmatrix}
\frac{\partial y}{\partial x_1} &
\frac{\partial y}{\partial x_2} &
\cdots &
\frac{\partial y}{\partial x_n}
\end{bmatrix}
$$

2. 标量对行向量求导，得到一个行向量
$$
\frac{\partial y}{\partial x} =
\begin{bmatrix}
\frac{\partial y}{\partial x_1} &
\frac{\partial y}{\partial x_2} &
\cdots &
\frac{\partial y}{\partial x_n}
\end{bmatrix}.
$$
3. 矩阵之间的求导相对复杂，可以自行查找

### 1.2 自动求导 Automatic Differentiation

也称作 Auto Gradient

自动求导指的是利用链式法则在计算图上自动传播梯度

#### 1.2.1 Forward Mode AD（正向模式）

- 从输入开始，向前传播导数
- 每次操作计算其对输入的 Jacobian，乘上前一个梯度

- 特别适合 *输入维度小、输出维度大* 的情况

- 神经网络场景（标量 loss）效率低

#### 1.2.2 Reverse Mode AD（反向模式 Backprop）

**这是深度学习的最主要的方式**

- 正向先记录计算图
- 反向从标量 loss 开始，用链式法则反传梯度

- 对标量 loss 非常高效
- 单次反向传播能计算对所有参数的梯度

- 需要存储计算图的中间值

#### 1.2.3 代码实现

在 pytorch 中使用的是反向传播函数获取梯度

```python
import torch

x = torch.arange(4.0)
xx.requires_grad_(True)  # 等价于x=torch.arange(4.0,requires_grad=True)
y.backward()
x.grad # 这个就是梯度
```
