# 数据处理

> 写一点官方文档中没有的，或者对自己来说相对更新的知识的内容

## 1 微积分

### 1.1 矩阵相关

1. 列向量对标量求导，得到一个行向量

$$
\frac{\partial y}{\partial x} =
\begin{bmatrix}
\frac{\partial y}{\partial x_1} &
\frac{\partial y}{\partial x_2} &
\cdots &
\frac{\partial y}{\partial x_n}
\end{bmatrix}
$$

2. 标量对行向量求导，得到一个行向量
$$
\frac{\partial y}{\partial x} =
\begin{bmatrix}
\frac{\partial y}{\partial x_1} &
\frac{\partial y}{\partial x_2} &
\cdots &
\frac{\partial y}{\partial x_n}
\end{bmatrix}.
$$
3. 矩阵之间的求导相对复杂，可以自行查找

### 1.2 自动求导 Automatic Differentiation

也称作 Auto Gradient

自动求导指的是利用链式法则在计算图上自动传播梯度

#### 1.2.1 Forward Mode AD（正向模式）

- 从输入开始，向前传播导数
- 每次操作计算其对输入的 Jacobian，乘上前一个梯度

- 特别适合 *输入维度小、输出维度大* 的情况

- 神经网络场景（标量 loss）效率低

#### 1.2.2 Reverse Mode AD（反向模式 Backprop）

**这是深度学习的最主要的方式**

- 正向先记录计算图
- 反向从标量 loss 开始，用链式法则反传梯度

- 对标量 loss 非常高效
- 单次反向传播能计算对所有参数的梯度

- 需要存储计算图的中间值

#### 1.2.3 代码实现

在 pytorch 中使用的是反向传播函数获取梯度

```python
import torch

x = torch.arange(4.0)
xx.requires_grad_(True)  # 等价于x=torch.arange(4.0,requires_grad=True)
y.backward()
x.grad # 这个就是梯度
```

## 2 Numpy

numpy 中有一些常见的操作，需要特别的掌握

### 切片算法

表示在这个维度中取某一串索引的数据，同时其他维度的形状保持原来的形状

这个索引的范围是 **左闭右开**

```python
# shape = (200, 20)
for i in range(max_degree):
    # 为了防止过大的梯度,一般在次方分母添加阶乘, 列方向的下标代表
    poly_features[:, i] /= math.gamma(i + 1) # gamma(n)=(n-1)! 
```

这里对行不进行任何的切片，表示所有的行都执行这个操作，另外列取了某一个列

同时除以一个标量阶乘，这里利用了广播机制，对每一个数值都进行了除法



### 广播机制

在矩阵元素进行逐元素运算的时候，可以自动扩展矩阵的一种机制

广播条件是：要么两个维度相等（也就不需要扩展了），要么其中一个为 1

#### 标量与数组

```python
a = np.array([1, 2, 3])
b = 2
a + b
```

标量会被自动扩展为 `[2, 2, 2]`进行逐个的加法运算

#### 行向量+列向量

```python
a = np.array([[1, 2, 3]])     # shape (1,3)
b = np.array([[1], [2], [3]]) # shape (3,1)
a + b 
# array([[2, 3, 4],
#        [3, 4, 5],
#        [4, 5, 6]])
```

#### 2D 广播

```python
# 1. 生成指数序列 [0, 1, 2, ..., max_degree-1]
exponents = np.arange(max_degree)          # shape = (20, 1)
# 2. 调整成形状 (1, max_degree)，方便广播
exponents = exponents.reshape(1, -1)       # shape = (1, 20)
# 3. 对 features 的每一行做不同次方（使用广播机制）
poly_features = np.power(features, exponents)   # shape = (200, 20)
# 也就是把 200 个数字横向广播 20次，每一个都和 arange(1, 20) 的数字进行一次指数运算
```

#### 自动扩展 batch

```python
x = torch.randn(32, 3, 224, 224)  # batch size 32
bias = torch.randn(3, 1, 1)       # per-channel bias
y = x + bias                      # 自动广播
```

tensor 中会逐个 **从右到左** 进行维度的对齐，左边缺失的维度默认是1

```
x:    32    3   224   224
bias:  1    3     1     1
```

根据广播机制检查条件，要么相等，要么有一个是 1，然后把 1 的那个维度扩展到对应的维度大小下
