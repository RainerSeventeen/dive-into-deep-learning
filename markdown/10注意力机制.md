#  注意力机制

> 时间紧，功利一点直接开始这一章节

## Attention 机制

### 注意力机制

我们把线索分为 随意线索和不随意线索，随意线索指的是“你想要做什么”，是注意力的重点，不随意线索指的是环境的一些东西，例如你自然而然的一些注意到的东西，和主观无关

- Query: 一个随意线索
- Value: 输入的一个数值
- Key: 不随意线索

### 注意力池化

Attention Pooling 会有偏向性地选择某一些输入，例如一个很经典的核回归函数
$$
f(x) = \sum_{i=1}^{n} 
\frac{K(x - x_i)}
{\sum_{j=1}^{n} K(x - x_j)}
\, y_i
$$
在这里面 $w_i(x) = \frac{K(x-x_i)}{\sum_j K(x-x_j)}$ 代表权重，核心思想是基于相似度的加权平均，越接近$x$ (查询值)的  $x_i$ , 其对应的 $y_i$ 会有更高的权重

如果应用高斯核 $K(u) = \frac{1}{\sqrt{2\pi}} \exp\left(-\frac{u^2}{2}\right)$ 则最后可以得到
$$
f(x)
=
\sum_{i=1}^{n}
\operatorname{softmax}\!\left(
-\frac{1}{2}(x - x_i)^2
\right)
\, y_i
$$
为了应用到机器学习，我们要加入一个权重让这个操作可以学习，于是就得到了：
$$
f(x)
=
\sum_{i=1}^{n}
\operatorname{softmax}\!\left(
-\frac{1}{2}\big((x - x_i)w\big)^2
\right)
\, y_i
$$

## 注意力分数

### 一维

$$
f(x) = \sum_i \alpha(x, x_i) y_i 
= \sum_{i=1}^{n} \operatorname{softmax}\!\left(-\frac{1}{2}(x - x_i)^2\right) y_i
$$

对于这个公式，其中 $\alpha(x, x_i)$ 是注意权重，$-\frac{1}{2}(x - x_i)^2$ 表示的是注意力分数

![](https://zh.d2l.ai/_images/attention-output.svg)

 正如这张图所展示的，将 Query 扔进来，和每一个 Key 计算一次注意力分数函数，在上边的例子里是 $-\frac{1}{2}(x - x_i)^2$ 

之后放进 Softmax 计算为权重，最后和 Values 相乘计算加权和，得到最终的输出

### 向量化

继续将 Query 升维得到一个向量 $\mathbf{q} \in \mathbb{R}^{q}$ , 另有 $m$ 个键值对 $\left(\right. \mathbf{k}_{1} , \mathbf{v}_{1} \left.\right) , \ldots , \left(\right. \mathbf{k}_{m} , \mathbf{v}_{m} \left.\right)$ , 其中 $\mathbf{v}_{i} \in \mathbb{R}^{v}$ ，那么注意力汇聚函数会变成
$$
f(\mathbf{q}, (\mathbf{k}_1, \mathbf{v}_1), \ldots, (\mathbf{k}_m, \mathbf{v}_m))
=
\sum_{i=1}^{m}
\alpha(\mathbf{q}, \mathbf{k}_i)\,\mathbf{v}_i
\in \mathbb{R}^{v}
$$
其中查询$\mathbf{q}$和键$\mathbf{k}_i$的注意力权重（标量）是通过**注意力评分函数** $a$ 将两个向量映射成标量，再经过softmax运算得到的：

$$
\alpha(\mathbf{q}, \mathbf{k}_i) = \mathrm{softmax}(a(\mathbf{q}, \mathbf{k}_i)) = \frac{\exp(a(\mathbf{q}, \mathbf{k}_i))}{\sum_{j=1}^m \exp(a(\mathbf{q}, \mathbf{k}_j))} \in \mathbb{R}.
$$

这里的 $a$ 就是注意力评分函数，用于计算 attention score

下面是两种最常见的分数计算公式

### 加性注意力

给定查询 $\mathbf{q} \in \mathbb{R}^q$ 和键 $\mathbf{k} \in \mathbb{R}^k$，加性注意力（additive attention）的评分函数为

$$
a(\mathbf q, \mathbf k) = \mathbf w_v^\top \text{tanh}(\mathbf W_q\mathbf q + \mathbf W_k \mathbf k) \in \mathbb{R},
$$

其中可学习的参数是 $\mathbf W_q\in\mathbb R^{h\times q}$ 、$\mathbf W_k\in\mathbb R^{h\times k}$和$\mathbf w_v\in\mathbb R^{h}$。这里的 $\text{tanh}$ 相当于一个激活函数，这个有点像一个输出大小为 1 的单隐藏层 MLP

这个注意力的优点是 Key Value Query 可以是任意的长度

### 缩放点积注意力

Scaled Dot-Product Attention 要求 Q 和 K 都是同样长度 ，此时可以有

$$
a \left(\right. \mathbf{q} , \mathbf{k} \left.\right) = \mathbf{q}^{\top} \mathbf{k} / \sqrt{d}
$$

这个时候就是没有学习的参数了，是最简单的注意力公式 

另外也有向量化版本，例如 $Q \in \mathbb{R}^{n \times d} \quad $ $K \in \mathbb{R}^{m \times d} \quad $ $V \in \mathbb{R}^{m \times v}$

此时计算的注意力分数为 $a(Q, K) = \frac{QK^{T}}{\sqrt{d}} \in \mathbb{R}^{n \times m}$

 对应注意力池化层 $f = \mathrm{softmax}\big(a(Q, K)\big) V \in \mathbb{R}^{n \times v}$ ，这里softmax 对每一行做一次，最后得到一个 $n \times m$ 的概率分布



